{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"hadoopProjects.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyM+K8GYZJSm7s1uMhhW6/E3"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"oy7-pQXpwI-G"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ct_0_rgsweDO"},"source":["# Using Hadoop's Core: HDFS and MapReduce\n","\n"]},{"cell_type":"code","metadata":{"id":"aCuiDQYVxC0N"},"source":["from mrjob.job import MRJob\n","from mrjob.step import MRStep\n","\n","class RatingsBreakdown(MRJob):\n","  def steps(self):\n","    return[\n","           MRStep(mapper=self.mapper_get_ratings, # this extracts the movieID and 1\n","                  reducer=self.reducer_count_ratings), # this count them up\n","           MRStep(mapper=self.reducer_sorted_output) # interates through each movie count \n","    ]\n","  def mapper_get_ratings(self, _, line):\n","    (userID, movieID, ratings, timestamp) = line.split('\\t')\n","    yield movieID, 1\n","  \n","  def reducer_count_ratings(self, _, line):\n","    yield str(sum(values)).zfill(5), key\n","  \n","  def reducer_sorted_output(self, count, movies):\n","    for movie in movies:\n","      yield movie, count\n","\n","if __name__ == '__main__':\n","  ratingsBreakdown.run()\n","\n","pass\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3yQO4No12dtZ"},"source":["# Programming Hadoop with Pig\n","\n","## PIG Latin: Thing i can do with Pig\n","\n","\n","1.   LOAD, STORE, DUMP\n","  *   STORE ratings INTP 'outRatings' USING PigStorage(':');\n","2.   FILTER, DISTINCT, FOREACH/GENERATE, MAPREDUCE, STREAM, SAMPLE\n","3.   JOIN, COGROUP, GROUP, CROSS, CUBE\n","4.   ORDER, RANK, LIMIT\n","5.   UNION, SPLIT\n","\n","## Diagnostics\n","\n","*   DESCRIBE\n","*   EXPLAIN\n","*   ILLUSTRATE\n","\n","## User Defined Functions(UDFs)\n","\n","*   REGISTER\n","*   DEFINE\n","*   IMPORT\n","\n","## Some other functions and loaders\n","\n","*   AVG, CONCAT, COUNT, MAX, MIN, SIZE, SUM\n","*   PigStorage\n","*   TextLoader\n","*   JsonLoader\n","*   AvroStorage\n","*   ParquetLoader\n","*   OrcStorage\n","*   HBaseStorage\n","\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"mujTQjfH0v7y"},"source":["# Creaating a relation name \"ratings\" with a given schema\n","\n","rating= LOAD 'user/maria_dev/ml_100k/u.data' AS # loads up the movie rating data, AS - gives the data a schema\n","(userID:int, movieID: int, ratingID:int,RatingTime:int); \n","\n","metadata = LOAD 'user/maria_dev/ml_100k/u.item' USING # this fie contains the actual name.  Hence, metadata will assiciate MovieID with movieTitle\n","PigStorage('|') AS (movieID:int, movieTitle:chararray, releaseDate:chararray, releaseDate:chararray, videoRelease:chararray, imdbLink:chararray);\n","\n","nameLookup = FOREACH metadata GENERATE movieID, movieTitle,ToUnixTime(ToDate(releaseDate, \"dd-MMM-yyyy\")) AS releaseTime;  # FOREACH GENRATE is going to let us generate a new relation that would contain the following stuff\n","\n","# Group by\n","ratingsByMovie = GROUP ratings By movieID; # this creates a bag in pig of all the raings associated with a movieID\n","\n","avgRatings = FOREACH ratingsByMovie GENERATE group AS movieID, AVG(ratings.rating) AS avgRating; # this goes across all the ratings and averages it up\n","\n","# FILTER\n","fiveStarMovies = FILTER avgRatings BY avgRating > 4.0;\n","\n","# JOIN - we need to join in the movie names \n","fiveStarWithData = JOIN fiveStarMovies By movieID, nameLookup BY movieID;\n","\n","# ORDER BY\n","oldestFiveStarMovies = ORDER fiveStarWithData BY nameLookup::releaseTime;\n","\n","DUMP oldestFiveStarMovies;\n","\n","pass\n","  \n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uuoxGRmEXUc7"},"source":["# Another Pig problem to solve\n","\n","\n","*   Find all movies with an average rating less than 2.0\n","*   Sort them by total number of ratings\n","\n"]},{"cell_type":"code","metadata":{"id":"KG_69uOvKuzX"},"source":["rating= LOAD 'user/maria_dev/ml_100k/u.data' AS # loads up the movie rating data, AS - gives the data a schema\n","(userID:int, movieID: int, ratingID:int,RatingTime:int); \n","\n","metadata = LOAD 'user/maria_dev/ml_100k/u.item' USING # this fie contains the actual name.  Hence, metadata will assiciate MovieID with movieTitle\n","PigStorage('|') AS (movieID:int, movieTitle:chararray, releaseDate:chararray, releaseDate:chararray, videoRelease:chararray, imdbLink:chararray);\n","\n","nameLookup = FOREACH metadata GENERATE movieID, movieTitle  # FOREACH GENRATE is going to let us generate a new relation that would contain the following stuff\n","\n","# Group by\n","groupedRatings = GROUP ratings By movieID; # this creates a bag in pig of all the raings associated with a movieID\n","\n","avgRatings = FOREACH groupedRatings GENERATE group AS movieID, AVG(ratings.rating) AS avgRating,\n","COUNT(ratings.rating) AS numRatings; # this goes across all the ratings and averages it up\n","\n","# FILTER\n","badMovies = FILTER avgRatings BY avgRating > 2.0;\n","\n","# JOIN - we need to join in the movie names \n","namedbadMovies = JOIN badMovies By movieID, nameLookup BY movieID;\n","\n","\n","finalResults = FOREACH namedbadMovies GENERATE nameLookup::movieTitle AS movieName,\n","badMovies::avgRating AS avgRating, badMovies::numRatings AS numRatings;\n","\n","# ORDER\n","finalResultsSorted = ORDER finalResults BY numRatings DESC;\n","\n","DUMP finalResultsSorted;"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0QaUFZYpe8v5"},"source":["# Hive\n","\n","## Why Hive?\n","\n","*   Uses familiar SQL syntax(HiveQL)\n","*   Interactive\n","*   Scalable - works with Big Data on a cluster and it is really most appropriate for warehouse applications.\n","*   Easy OLAP queries - Way easier than writing MapReduce in Java\n","*   Highly Optimized\n","*   Highly extensible:\n","  *   User define function\n","  *   Thrift server\n","  *   JDBC/ ODBC driver\n","\n","## Why Not Hive?\n","\n","*   High Latency - Not Appropriate fir OLTP\n","*   Stores data de-normalized\n","*   SQL is limited in what it can do;\n","  *   Pig, Spark allows for more complex stuff\n","*   No Transactions\n","*   No record-level updates, inserts and deletes\n","\n","## HiveQL\n","\n","*   It is pretty much MySQL with some added extentions\n","*   for example views:\n","  *   Can store results of a query inot a \"view\", whihc subsequent queries can use as a table.\n","*   Allows you to specify how structured data is stored in a partition.\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"cL6HUjVonp0M"},"source":["### Working with Hive to find the most popular movie in our dataSet."]},{"cell_type":"code","metadata":{"id":"4xksDNPudPlv"},"source":["CREATE VIEW topMovieIDs AS  # create a table name top movieID with 2 columns\n","SELECT movieID, count(movieId) as ratingCount\n","FROM ratings\n","GROUP BY movieID\n","ORDER BY ratingCount DESC; # shows data in decending order\n","\n","SELECT n.title, ratingCount\n","FROM topMovieIDs t JOIN names n ON t.movieID=n.movieID\n","\n","pass"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VqzaaV4UrF9Q"},"source":["## Hive - \n","*   Schema on Read: \n","  *   Hive maintain a \"metastore\" that imparts a struture you define on unstructured data that is stored on the HDFS, etc.\n","*   Where is the Data:\n","  *   LOAD DATA - MOVES data from a distributed filesystem into Hive\n","  *   LOAD DATA LOCAL - COPIES data from you local filesystem into Hive\n","  *   Managed vs External tables\n","    CREATE EXTERNAL TABLE IF NOT EXISTS rating(\n","      userID INT\n","      movieID INT\n","      rating INT\n","      time INT\n","    ROW FORMAT DELIMITED FIELDS TERMINATED BY '\\t'\n","    LOCATION'data/ml-100k/u.data'\n","\n","in this case the external table if Hive is suing it and we drop it.  the original copy at the stored locaton will still be in tact.  Managed LOAD DATA if a table is dropped it si gone forever.  Hence, Hive does not take ownership of external table.\n","\n","*   Partinioning: \n","  *   you can store data in partioned subdirectories.\n","  * Huge optimization if your queries is only for certain partions\n","\n","  CREATE TABLE customers(\n","    name STRING,\n","    address STRUCT <street: STRING, city:STRING, zip:INT>\n","  )\n","  PARTITION BY (country STRING);\n","\n","      .../customers/country=CA/\n","      .../customer/country=GB/\n","\n","# Ways to use Hive\n","*   Interactive via Hive>prompt/Command Line Interface(CLI)\n","*   Saved query files:\n","  * hive -f /somepath/queries.hql\n","*   Through Ambari/Hue\n","*   Through JDBC/ODBC server\n","*   Through Thrift Service \n","  *   Note that Hive is not suitable for OLTP\n","* Via Oozie\n"]},{"cell_type":"code","metadata":{"id":"i3muDB71pAzu"},"source":["# Find the movie withthe Highest average rating\n","# Only consider movie with more than 10 ratings\n","\n","CREATE VIEW IF NOT EXISTS avgRatings AS \n","SELECT movieID, AVG(rating) as avgRating, COUNT(movieID) as ratingCount\n","FROM rating\n","GROUP BY movieID\n","ORDER BY avgRating DESC;\n","\n","SELECT n.title, avgRating\n","FROM avgRatings t JOIN names n ON t.movieID=n.movieID\n","WHERE ratingCount >10;\n","\n","pass\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PsFnwjMtDmiK"},"source":["# Integrating MySQL with Hadoop\n","\n","*   scoop can handle Big data.\n","*   It actually kicks of MapReduce to handle importing or exporting your data\n","*   Hence, the mappers(distributing data) taking data from MySQL, is going to talk to my HDFS and create a set of table where we can then use Hive or pig on it\n","*   Scoop: Import Data from MySQL to HDFS\n","  *   scoop:\n","import --connectjdbc:mysql://localhost/movielens --driver.com.mysql.jdbc.Drivers --table movies\n","*   Scoop: Import data directly into Hive\n","  *   scoop import --connectjdbc:mysql://localhost/movielens --driver.com.mysql.jdbc.Drivers --table movies --hive-import\n","*   Another thing about Scoop is that you can do incremental imports:\n","  *   you can keep your relational database and Hadoop in sync\n","  *   --check-column and --last-value\n","*   Scoop: Export data from Hive to MySQL:\n","  *   scoop export(table must be created ahead of time) --connect jdbc:mysql://localhost/movielens -m 1 --driver.com.mysql.jdbc.Drivers --tables export_movies --export-dir apps/hive.warehouse/movies--input-fields-terminated-by'\\0001'\n","*   Target table must already exist in MySQL, with columns in expected order.\n"]},{"cell_type":"markdown","metadata":{"id":"u0UPiYIgo0aO"},"source":["MySQL does come alread install in HortonworksSandbox\n","\n","*   to login to MySQL:\n","*   mysql -u root -p\n","*   password: hadoop\n","*   Creating a movielens database then i would import some data into it:\n","1.   create database movielens;\n","2.   show databases; # you will now see the dbs\n","3.   exit\n","4.   downloading the data: wget http://media.sundog.soft.com/hadoop/movielens.sql.\n","5.   I need to log back inot MySQL\n","6.   There are some internation characters in this dataset i need to deal with.\n","7.   SET NAMES 'utf8';\n","8.   SET CHARACTER SET utf8;\n","9.   I need to tell it to use the dbs i created earlier:\n","  *   use movielens;\n","10.   source movielens.sql\n","11.   show tables; # will get back the dbs\n","12.   SELECT * from movies limit 10; # to get the first 10 rows\n","13.   describe ratings; \n","\n","using MySQL we can figure out he top rated mvies and thier titles\n","\n","SELECT movies.title, COUNT(ratings.movie_id) AS ratingCount\n","FROM movies\n","INNER JOIN ratings\n","ON movies.id = ratings.movies_id\n","GROUP BY movies.title\n","ORDER BY ratingCount;\n","\n","14.   exit - to get back \n","\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"dNXcf6STiajE"},"source":["GRANT ALL PRIVILIDGES ON movielens.*to''@'localhost';\n","exit;\n","\n","pass"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NrlONoDsda6B"},"source":["# Import Data from MySQL into HDFS"]},{"cell_type":"code","metadata":{"id":"9xhhddvHXrV1"},"source":["sqoop import --connectjdbc:mysql://localhost/movielens --driver com.mysql.jdbc.Driver --table movies -m 1 # this means we connecting to a mysql database\n","# and it dumps all of our data inot the HDFS\n","\n","pass\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TS2rF0eec_4i"},"source":["# Putting Data into Hive imported from MySQL"]},{"cell_type":"code","metadata":{"id":"ha9rrJHvakz9"},"source":["sqoop import --connectjdbc:mysql://localhost/movielens --driver com.mysql.jdbc.Driver --table movies -m --hive-import # this will import the data into hive\n","pass"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0vbeuQP6c5Fo"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Gq6Ra7bsds0z"},"source":["# Using Scoop to export data from Hadoop to MySQL\n","Note. yiou need to make sure the table exist ahead of time in order to recieve the data\n","\n","*   Login: mysql - u root -p;\n","*   use movielens;\n","*   CREATE TABLE exproted_movies (id INTEGER, title VARCHAR(255), releaseData Date); # this gives us the table we need to recieve the data\n","*   exit\n","# export data from Hive back into MySQL\n","scoop export --connectjdbc:mysql://localhost/movielens -m 1 --driver com.mysql.jdbc.Driver --table exported_movies --export-dir /apps/hive.warehouse/movies--input-fields-terminated-by'\\0001'\n","\n"]},{"cell_type":"markdown","metadata":{"id":"9x2eDs19AlNS"},"source":["# NoSQL\n"]},{"cell_type":"markdown","metadata":{"id":"MSHBL7w-As3I"},"source":["## HBase\n","\n","What not to do when dealing with planet size data:\n","Scaling up MySQL etc. to massive load requires extreme measures\n","\n","*   Denormalizing\n","*   Caching layers\n","*   Mater/slave setups\n","*   Sharding\n","*   Materialzed views\n","*   Remvoing Stored procedures\n","\n","### What is Apache HBase?\n","*   It is a non-relational, scalable database built on HDFS\n","*   It is an open source implementation of Google Big Table with a few minr changes.\n","*   It does not have a query language\n","*   It use an API to carry out CRUD operation\n","  *   Create\n","  *   Read\n","  *   Update \n","  *   Delete\n","### HBase data model\n","*   Fast access to any given row\n","*   A ROW is referenced by a unique KeY\n","*   Each ROW has a small number of COLUMN FAMILIES\n","*   A COLUMN FAMILY may contain arbitrary COLUMNS\n","*   You can have a large number of COLUMNS in a COLUMN FAMILY\n","*   Each CELL have many VERSIONS with a given timestamps\n","*   Sparse data is A-OK missing columns ina row consume no storage.\n","\n","### Some way to access HBase\n","*   HBase shell\n","*   Java API\n","  *   Wrappers for Python, Scala, etc\n","*   Spark, Hive, Pig\n","*   REST service\n","*   Thrift Service\n","*   Avro Service\n","\n","### Gonna create an HBase tabel with Python via REST\n","Plan:\n","*   Create an HBase table for movie rating by user\n","*   Then quickly query it for individual users\n","*   This is a good example of Sparse data.\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"PK7RhPCBhAdS"},"source":["from starbase import Connection\n","\n","c = Connection(\"127.0.0.1\", \"8000\") # Create the connection on port 8000 which is the port i open to the virtualBox and the REST server will be operating on this port\n","\n","ratings = c.table(\"ratings\") # creating a table called ratings\n","\n","if (ratings.exists()):\n","  print(\"Dropping existing table\\n\")\n","  ratings.drop()\n","\n","ratings.create(\"rating\") # this creates a family table called rating.  \n","\n","print(\"Parsing the ml-100k ratings data ...\\n\")\n","ratingFile = open(\"c:/downloads/ml-100k/ml-100k/u.data\", \"r\") # open file in read only fashion\n","\n","batch = ratings.bach() # instead of adding in a row at a time I can do it in batches.  I am creating a batch object from my raings table\n","\n","for line in ratingFile: # i will keep updates the bach with new rows\n","  (userID, movieID, rating, timestamp) = line.split()\n","  batch.update(userID, {\"rating\": {movieID:rating}})\n","\n","ratingFile.close()\n","\n","print(\"Commiting ratings data to HBase via RESTservices\\n\")\n","batch.commit(finalize=True)\n","\n","print(\"Get back raitngs for some users ...\\n\")\n","print(\"Ratings for user ID 1: \\n\")\n","print(ratings.fetch(\"1\"))\n","print(\"Ratings for user ID 33:\\n\")\n","print(raings.fetch(33))\n","\n","ratings.drop()\n","\n","pass"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8d8t_DPCaCTl"},"source":["# Integrating Pig with HBase (again is good for Big data)\n","\n","Plan:\n","\n","*   Must create table ahead of \n","*   Relations must have a unique key for the first column, followed by subsequent columns as you want them saved in HBase\n","*   USING clause allows you to store into an HBase table\n","*   Can work at Scale - HBase is transactional on rows\n","\n","In Hbase, I am goignto create a table:\n","\n","create 'users', 'userinfo' # hit enter and the table will be created\n","\n","exit\n","\n"]},{"cell_type":"code","metadata":{"id":"aj91PQE4URc2"},"source":["ratings = LOAD'/user/maria_dev/ml-100k/u.user'\n","USING PigStorage(\"|\")\n","AS (userID:int, age:int, gender:chararray, occupation:chararray, zip:int);\n","\n","STORE rating INTO 'hbase://users'\n","USING org.apache.pig.backend.hadoop.hbase.HBaseStorage(\n","    'userinfo:age, userinfo:gender, userinfo:occupation, userinfo:zip'\n",")\n","hbase.pig(end)\n","\n","pig hbase.pig # runs the code\n","disable 'users' # to close it off\n","drop 'users' to drop it from the dbs\n","exit # to exit out of the shell\n","and finally close of HBase \n","pass"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wG29zt1ZpaHX"},"source":["# Cassandra \n","This is a distributed database with no single point of failure.  \n","\n","## Cassandra -NoSQL ith a twist\n","\n","*   Unlike HBase, there is no master node at all, every node runs exactly the same software and performs the same functions\n","*   Data Model is similar to BigTable/HBase\n","*   It is non-relational, but has a liited CQL query language as its' interface.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"-tXWQKT_sxFu"},"source":["## Cassandra's Design Choices\n","*   The CAP theorem says you can only have 2 out of 3:consistency, availability, partition-tolerance.\n","  *   Partition-Tolerance is a requirement of \"Big Data\".  SO you really only get a choice between consistency or availability.\n","*   Cassandra favors availability over consistency.\n","  *   It is \"eventually consistent.\"\n","  *   But you can specify your consistency requirements as part of your request.  Hence, it is \"tunable consistency.\"\n","\n","## Cassandra and Cluster\n","*   Cassandra's great for fast access to rowsof information\n","*   Get the best of both worlds by replicating to another ring that is used for analytics and spark integration.  \n","\n","## CQL\n","*   Cassandra's API is CQL, which makes it easy to look like existing database drivers to application.\n","*   CQL is like SQL, but with some big limitations!\n","  *   NO JOINS\n","    *   your data must be de-normalized\n","    *   so it is still non-relational\n","  *   All queries must be on the same primary key:\n","    *   Second indices are supported, but...\n","*   CQLSH can be used on command line to create tables,\n","*   All tables must be in a *keyspace* - keyspaces are like databases."]},{"cell_type":"markdown","metadata":{"id":"uxYY-i6Sy91q"},"source":["# Cassandra and Spark\n","*   DataStax offers a spark-cassadra connector.\n","*   Allows you to read and write Cassandra Tables as DatFrames\n","*   Is smart about passing queiries on those DataFrames down to the appropriate level.\n","*   Uses cases:\n","  *   Use Spark for analytics and data stored in Cassandra\n","  *   Use Spark to transfomr data and store it inot Cassandra for transactional use.\n","\n","### Plan:\n","*   Install Cassandra on Hadoop Node\n","*   Setup a table for movielens users\n","*   Write into that table and query it from Spark"]},{"cell_type":"code","metadata":{"id":"Sw-3cERwfP-G"},"source":["cqlsh> CREATE KEYSPACE movielens WITH application = {'class': 'SimpleStrategy','replication_factor': '1'} AND durable_writes=true;\n","cqlsh> Use movielens;\n","cqlsh:movielens> CREATE TABLE users(user_id int, age int, gender text, occupation text, zip text, PRIMARY KEY(user_id));\n","cqlsh:movielens> DESCRIBE TABLE users # it will show you the results \n","pass"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"49Nu8lNCNwwS"},"source":["## Write Spark output into Cassandra\n"]},{"cell_type":"code","metadata":{"id":"hAVhn-D3NVRP"},"source":["from pyspark.sql import SparkSession\n","from pyspark.sql import functions\n","from pyspark.sql import Row\n","\n","\n","def parseInput(line):\n","  fields = line.split(\"|\")\n","  return Row(user_id=int(fields[0]), age=int(fields[1]), gender=fields[2], occupation=fields[3], zip=fields[4])\n","\n","if __name__=\"__main__\":\n","  # Create a spark Session\n","  spark = SparkSession.builder.appName(\"CassandraIntegration\").master(\"spark.cassandra.connection.host\", \"127.0.0.1\").getOrCreate()\n","\n","  # Get the raw data\n","  lines = spark.sparkContext..textFile(\"hdfs:///user/maria_dev/ml-100k/u.user\")\n","  # Convert it to a RDD of row objects with(user_id, age, gender, occupation, zip)\n","  users = lines.map(parseInput)\n","  # Convert that into a DataFrame\n","  usersDataset = spark.createDataFrame(users)\n","\n","  # Write this into Cassandra\n","  usersDataset.write\\\n","  .format(\"org.apache.spark.sql.cassandra\")\\\n","  .mode(\"apend\")\\\n","  .options(table=\"users\", keyspace=\"movielens\")\\\n","  .save()\n","\n","  # Read it back from Cassandra into a new DataFrame\n","  readUsers = spark.read\\\n","  .format(\"org.apache.spark.sql.cassandra\")\\\n","  .options(table=\"users\", keyspace=\"movielens\")\\\n","  .load()\n","\n","  readUsers.createOrReplaceTempView(\"users\")\n","\n","  sqlDf = spark.sql(\"SELECT * FROM users WHERE age < 20\")\n","  sqlDF.show()\n","\n","  # Stop the session\n","  spark.stop()\n","  pass\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1KisZjf2T47p"},"source":["# to import the file \n","wget(web address of the file)\n","# telling spark what version I am using\n","export SPARK_MAJOR_VERSION=2\n","spark-submit --packages datastax:spark-cassandra-connector:2.0.0-m2-s_2.11 CassandraSpark.py\n","\n","pass\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"X7SSFc5TY4Bg"},"source":["finally after runing \n","\n","exit and stop Cassadra(service cassandra stop)"]},{"cell_type":"markdown","metadata":{"id":"8tDTLVvias8p"},"source":["# MongoDB\n","\n","### Integrating Spark with MongoDB\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"RzYVKqbGsb8I"},"source":["from pyspark.sql import SparkSession\n","from pyspark.sql import functions\n","from pyspark.sql import Row\n","\n","\n","def parseInput(line):\n","  fields = line.split(\"|\")\n","  return Row(user_id=int(fields[0]), age=int(fields[1]), gender=fields[2], occupation=fields[3], zip=fields[4])\n","\n","if __name__=\"__main__\":\n","  # Create a spark Session\n","  spark = SparkSession.builder.appName(\"MongoDBIntegration\").getOrCreate()\n","\n","  # Get the raw data\n","  lines = spark.sparkContext..textFile(\"hdfs:///user/maria_dev/ml-100k/u.user\")\n","  # Convert it to a RDD of row objects with(user_id, age, gender, occupation, zip)\n","  users = lines.map(parseInput)\n","  # Convert that into a DataFrame\n","  usersDataset = spark.createDataFrame(users)\n","\n","  # Write this into MongoDB\n","  usersDataset.write\\\n","  .format(\"com.mongodb.spark.sql.DefaultSource\")\\\n","  .mode(\"apend\")\\\n","  .options(\"uri\", \"mongodb://127.0.0.1/movielens.users\")\\\n","  .save()\n","\n","  # Read it back from Cassandra into a new DataFrame\n","  readUsers = spark.read\\\n","  .format(\"com.mongodb.spark.sql.DefaultSource\")\\\n","  .options(\"uri\", \"mongodb://127.0.0.1/movielens.users\")\\\n","  .load()\n","\n","  readUsers.createOrReplaceTempView(\"users\")\n","\n","  sqlDf = spark.sql(\"SELECT * FROM users WHERE age < 20\")\n","  sqlDF.show()\n","\n","  # Stop the session\n","  spark.stop()\n","  pass"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IHYgfk4csdEa"},"source":["# Using sandbox I am going to run this code\n","less MongoSpark.py\n","export SPARK_MAJOR_VERSION=2\n","spark-submit --packages org.mongodb.spark:mongo-spark-connector_2.11:2.0.0 MongoSpark.py\n","\n","pass"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I5XQZQHHQBOW"},"source":["# Setting up APache Drill\n","*   wget http://archive.apache.org/dist/drill.drill-1.12.0/apache-drill-1.12.0.tar.gz\n","*   tar -xvf apache-drill-1.12.0.tar.gz #this will decompress the entire package\n","*   cd apache-drill-1.12.0 # after the package is decompressed we can cd into the directories.\n","*   bin/drillbit.sh start -Ddrill.exec.port=8765 # this will start up the servers for Apache drill. Also we need to overide the default port and open a port that is open to the outside world so that we can communicate with drill(HDP is running on a docker container so opening a port in virtula box won't work). Hence, this will start up drillbit on the given port.  just need to pull up my browser and start working on it going to use 127.0.0.1:8765\n","*   Once on drillbit click on storage and enable hive and mongo if it is disable.\n","*   Finally hit the update button next to Hive and we are connected."]},{"cell_type":"markdown","metadata":{"id":"VgjOtnPwgajn"},"source":["### Query across multiple database with drill.\n","The nice thing about drill it is standard SQL database.  Hence, we query it using SQl as we would do for any database."]},{"cell_type":"code","metadata":{"id":"xMW_id0MVZcA"},"source":["SHOW DATABSES; # WILL SHOW ALL THE DBS.\n","SELECT * FROM hive.movielens.ratings LIMIT 10; # so this will give me back the first 10 coloumns from hive\n","SELECT * FROM mongo.movielens.users LIMIT 10;  # so this will give me back the first 10 coloumns from mongo\n","SELECT u.occupation, COUNT(*) FROM hive.movielens.ratings r JOIN mongo.movielens.users u ON r.user_id = u.user_id GROUP BY u.occupation # this queries to different dbs, jojns and group result\n","bin/drillbit.sh stop # this shuts down drillbit\n","pass"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uVT5UahYkpD0"},"source":["## Apache Phoenix\n","\n","*   A SQL driver for HBase that supports transactions\n","*   Fast, low-latency - OLTP support\n","*   Originally developed by salesforce, then open-source\n","*   Exposes a JDBC connector for HBase\n","*   SUpports secondary indices and user-defined functions\n","*   Integrates with MapReduce, Spark, Hive, Pig, and Flume\n","\n","### Using Phoenix\n","*   Command-Line Interface(CLI)\n","*   Phoenix API for Java\n","*   JDBC driver (thick client)\n","*   Phoenix Query Server(PQS)(thin client)\n","  *   Intended to eventually handle non-JVM access\n","*   JARs for MapReduce, Spark, Hive, Pig and Flume\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"NGJ9CVKnqK-Y"},"source":["## Installing phoenix\n","*   Ensure HBase is running before installing Phoenix\n","*   yum install phoenix # this command install phoenix\n","*   python sqlline.py # this kicks of phoenix\n","*   !tables #  gives all the tables Phoenix knows about.  Also, phoenix runs on top of HBase and all queries again uses SQL.  Some commands like INSERT is not availabel but we Use UPSERT instead.  \n"]},{"cell_type":"markdown","metadata":{"id":"YlGLZSGirZVq"},"source":["## Gonna execute some sql commands\n","*   CREATE TABLE IF NOT EXIST us_population(state CHAR(2) NOT NULL, city VARCHAR NOT NULL, population BIGINT, CONSTRAINT my_pk PRIMARY KEY(state, city));\n"]},{"cell_type":"code","metadata":{"id":"apH4eCM5kAHi"},"source":[" UPSERT INTO US_POPULATION VALUES('NY', 'NEY YORK', '8143197'); # THIS WOULD INSERT THESE VALUES IN THE TABLE\n"," SELECT * FROM US_POPULATION; # RETURNS THE TABLE \n"," SELECT * FROM US_POPULATION WHERE STATE='NY';\n"," DROP TABLE US_POPULATION;\n"," !quit\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RzD1718a1H70"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"Jokx4x8q1IUx"},"source":["## Integrate Phoenix with Pig to go into HBase\n","*   python.sqlline.py # this takes us to our phoenix prompt\n","*   CREATE TABLE users(USERID INTEGER NOT NULL, AGE INTEGER, GENDER CHAR(1), OCCUPATION VARCHAR, ZIP VARCHAR CONSTRAINT pk PRIMARY KEY(USERID)); # this would give us the user table in HBase\n","*   !tables # to make sure the user table\n","*   !quit\n","Go back to sandbox and create the data\n","*   mkdir ml-100k\n","*   cd ml-100k/\n","*   wget (web address) to retrieve the data \n","*   cd..# back to home dir\n","\n","*   wget http://media.sundog-soft.com/hadoop/phoenix.py\n","  *   REGISTER /usr/hdp/current/phoenix-client/phoenix-client.jar\n","  *   users = LOAD'user/maria_dev/ml-100k/u.user'\n","  *   USING PigStorage('|')\n","  *   AS (USERID:int, AGE:int, GENDER:chararray, OCCUPATION:chararray, ZIP:chararray);\n","\n","  *   STORE users into 'hbase://users' using org.apache.phoenix.pig.PhoenixHBaseStorage('localhost', '-batchsize 5000');\n","\n","  *  occupations = load 'hbase://table/user/USERID,OCCUPATION' using org.apache.phoenix.pig.PhoenixHBAseLoader('localhost');\n","\n","  *   grpd = GROUP occupations BY OCCUPATION\n","  *   cnt = FOREACH grpd GENERATE group AS OCCUPATION, COUNT(occupations);\n","  *   DUMP cnt\n","  *   phoenix.pig (END)\n","\n","*   pig.phoenix.pig # this will run the code\n"]},{"cell_type":"markdown","metadata":{"id":"Vi3QCEmVFTJG"},"source":["# Presto\n","*  It is a lot like Drill\n","  *   It can connect to many different \"Big Data\" databases and data stores at once and query across them\n","  *   Familiar SQL syntax\n","  *   Optimizing for OLAP - analytical queries, data warehousing\n","  *   Developed and partially maintained by facebook\n","  *   Exposes JDBC, Command-line, and Tableau intefaces\n","  *   vs Drill, It has a cassandra connector\n","  *   Its good enough for Facebook, DropBox and AirBNB\n","  *   A single Presto query can combine data from multiple sources, allowing analitic across entire organisation.\n","  *   Presto breaks the False choice between having fast analytics using an expensive commercial solution or using a slow free solution requireing execessive hardware.  "]},{"cell_type":"markdown","metadata":{"id":"ZM8m3R6SLLnK"},"source":["## Install Presto and query Hive with it.\n","*   wget https://repo1.maven.org/maven2/com/facebook/presto/presto-server/0.243.1/presto-server-0.243.1.tar.gz\n","*   tar -xvf presto-server-0.243.1.tar.gz\n","*   cd presto-server-0.243.1.tar.gz\n","*   wget http://media.sundog-soft.com/hadoop/presto-hdp-config.tgz # Config files.\n","*   tar-xvf presto-hdp-config.tgz # this runs presto and creates the etc files\n","*   cd etc\n","\n","We need to get a command-line interface.  Goinhg to documentation:(make sure you are in the bin/presto server.\n","\n","*   wget https://repo1.maven.org/maven2/com/facebook/presto/presto-cli/0.243.1/presto-cli-0.243.1-executable.jar # run this to get the CLI\n","\n","*   mv presto-cli-0.243.1-executable.jar presto # rename the file\n","*   chmod +x presto # it is now an executable command and it would kick of the CLI withthe appropriate parameters\n","*   cd.. # takes me back to the top of the presto server directory\n","*   pwd # shows you where you are and what you have installed\n","*   bin/launcher start # launches the server\n","*   127.0.0.8090 # this would bring up our dash board showing our cluster overview\n","\n","*   bin//presto --server 127.0.0.1:8090 --catalog hive # running this aloows me to connect to the server.\n","\n","*   show tables from default;  # my raitngs table shows up in this case\n","*   select * from default.raings limit 10; # give me back the first 10 rows\n","*   select * from default.ratings where rating = 5 limit 10; returns all 5 star rating.   \n","*   select count (*) from default.ratings where rating=1; # this counts up all 1 star rating  (6110 one star rating)\n","*   quit # to end ;\n","* bin/launcher stop # stops presto;\n","\n","## Query both Hive and Cassandra using presto\n","*   scl enable python27 bash;\n","*   service cassandra start; # starts cassandra\n","*   notool enablethrift; # presto needs this to be able to communicate with cassandra.\n","*   cqlsh --cqlversion=\"3.4.0\";  # Loads up cassandra CLI\n","*   describe keyspaces; # movielens shows up\n","*   use movielens \n","*   describe tables;\n","*   select * stars from users limit 10;\n","Now i am going to connect this to presto together with the raitngs data that is living in Hive combing them to do a query.  In order to connect with presto we need to set up a configuration file for cassadra with presto before i can query the data :)  fro the presto server directory lets:\n","*   cd etc/catalog \n","*   vi cassandra.properties (hit \"I\" key to insert)\n","*   connector.name=cassadra\n","*   cassandra.connect-points=127.0.0.1\n","*   Esc\n","*   :wq # writes and quit this file\n","*   cd ../.. # moveup by 2 dir\n","*   bin/launcher start # starts to presto\n","*   bin/presto --server 127.0.0.1:8090 --catalog hive, cassandra\n","running query now\n","*   show tables from cassadra.movielens; # displays my \"users\" table\n","*   describe cassandra.movielens.users;\n","*   select * from cassandra.movielens.users limit 10;\n","*   select * from hive.default.ratings limit 10;\n","*   select u.occupation, count(*) from hive.default.rating r JOIN cassandra.movielens.users u on r.user_id = u.user_id group by u.occupation; # this groups all of the users for each unique occupation and count up the relevant ratings\n","*   quit\n","*   bin/launcher stop # stops presto\n","*   service cassandra stop\n","\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"G608DJtpjc0f"},"source":["# Feeding data into my cluster"]},{"cell_type":"markdown","metadata":{"id":"wyXa_sA0nMeA"},"source":["## Kafka\n","### Streaming\n","*   Lets you publish this data in real time to your cluster\n","  *   You can even process it in real time as your data comes in.\n","\n","*   Two problems:\n","  *   How to get data from many sources flowing into your cluster.\n","  *   Processing it when it gets there.\n","\n","##**Kafka**\n","*   Kafka is a general purpose publish/subscribe messaging system\n","*   Kafka servers stores all of the incoming mesages from publishers for some time, and pubishes them to stream of data called a topic.\n","*   Kafka cosumers subcribe to one or more topics, and recieved data as it's published\n","*   A stream/topic can have many consumers, all with their own position in the stream maintain.  \n","*  Its not jsut for Hadoop\n","\n","### How Kafka Scales\n","*   Kafka itself may be distributed amon many processes among many servers.\n","  *   WIll distribute the storage of stream data.\n","*   Consumers may also be distributed:\n","  *   Consumers of the same group will have messages distributed amoung them.\n","  *   Consumers of different groups will get their own copy of each message\n","\n","### Kafka Project Plan\n","*   Set up a topic\n","  *   Publich some data to it, and watch it get consumed\n","*   Set up a file connector\n","  *   Monitor a log file and publish additions to it.\n","\n","*   ./kafka-topic.sh --create --zookeeper sandbox.hortonworks.com:2181 --replication-factor 1 --partition 1 --topic fred # this creates our stream.\n","*   ./kafka-topic.sh --list --zookeeper sandbox.hortonworks.com:2181 --replication-factor 1 --partition 1 --topic fred # this creates our stream. # making sure it was created by checking it is there.\n","*   ./kafka-console-producer.sh --broker-list sandbox.hortonworks.com:6667 --topic fred  # this kafka producer will listen for keystroke data and braodcast any message on the topic \"fred\"\n","\n","Two msg is goign to be published:\n","\n","  this is a line of data\n","\n","  i am sending this on the fred topic\n","\n","Opening a second window on sandbox\n","*   cd /usr/hdp/current/kafka-broker/bin\n","*   ./kafka-consol-consumer.sh --boostrap-server sandbox.hortonworks.com:6667 --zookeeper locahost:2181 --topic-fred --from-beginning # as the kafka consumer when this code is kick of i will recieve the two line of data sent above. \n","\n","### Publishing web logs with kafka\n"," Going to use a built in Kafka connector to monitor a file and publish new lines to that file which then gets written out to another file somewhere lse(log processor).\n","\n"," Kafka has a file connector that is built in.  I just need to configure it.\n","\n","*   cd.. # out of the bin folder\n","*   cd conf # there are some sample configuration\n","*   Since these are Kafka file. I will make copies so that I can do what ever i want to them.  I am going to sopy them into my home dir\n","*   cp connect-standalone.properties ~/\n","*   cp connect-file-sink.properties ~/\n","*   cp connect-file-source.properties ~/\n","*   cd ~ # to edit them\n","\n","when editing the file:\n","to Insert text hit the I key.\n","to leave, hit esc.  then :wq\n","\n","\n","*   vi connect-standalone.properties # the bootstrap server needs to reflect the corrrect port and host(sandbox.howtonworks.com:6667).  then to write and quite \":wq\"\n","*   vi connect-file-sink.properties # file=/home/maria_dev/logout.txt \"(hit I to insert)\"  Changes the name of the file.  we also need to specifiy the topic Iwe are goignto listent too.  topics=log-test\n","*   vi connect-file-source.properties # file=/home/maria_dev/access_log_small_txt.  topic=log-test\n","\n","Yeehaa. everythig is configured!!\n","\n","wget http://media.sundog-soft.com//hadoop/access_log_small.txt\n","less access_log_small.txt # take a look at the results\n","\n","Going to set up a cosumer to print out what is going on that \"test-log\" topic\n","\n","./kafka-console-consumer.sh --bootstrap-server.sandbox.hortonworks.com:6667 --topic log-test --zookeeper localhost:2181\n","\n","*   cd /usr/hdp/current/kafka-broker/bin\n","*   ./connect.standalone.sh ~/ connect-standalone.properties connect ~/connect.file.source.properties ~/connect.file.sink.properties\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"85C9TN4-cyAa"},"source":["# Apache Flume (streaming)\n","## There are 3 components of a Fume agaent\n","*   Source\n","  *   Where is the data coming from\n","  *   Can optionally have Channel selectors and Interceptors\n","*   Channel\n","  *   How the data is transfered(via memory or files)\n","*   Sink\n","  *   Where the data is going \n","  *   Can be organized into sink Group\n","  *   A sink can only be connected to one channel\n","    *   Channel is notified to delete a message to delete a message once sink processes it.  \n"]},{"cell_type":"markdown","metadata":{"id":"eTuQTR4nvKvs"},"source":["## A single-node Flume configuration(setting up a flow)\n","\n","### Name the componenets on this agent\n","*   a1.sources = r1\n","*   a1.sinks = k1\n","*   a1.channels = c1\n","\n","### Describe and configure the source\n","*   a1.sources.r1.type = netcat # utility in unix that can listenin on a port for traffic\n","*   a1.sources.k1.bind = localhost # going to bind it to local host and list on port 4444\n","*   a1.sources.c1.port = 4444\n","\n","### Describe the sink\n","*   a1.sinks.k1.type = logger # writes everything to the log from flume\n","\n","### Use a channel which buffers events in memory\n","*   a1.channels.c1.type = memory\n","*   a1.channels.c1.capacity = 1000\n","*   a1.channels.c1.transactionCapacity = 100\n","\n","### Bind the source and the sink to the channel\n","*   a1.sources.r1.channels = c1\n","*   a1.sink.k1.channel = c1\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"0K9AXzJA2bzw"},"source":["Time to kick of flume\n","\n","*   cd /usr/hdp/current/flume-server/\n","*   bin/flume-ng agent --conf conf --conf-file ~/example.conf --name a1 -Dflume.root.logger=INFO, console #this means that every single logger and information that comes through is going to be print out.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"EEJk1Lml_nUc"},"source":["## Set up Flumes to monitor directory and store its data into HDFS\n","\n","### A single-node Flume configuration(setting up a flow)\n","\n","Name the componenets on this agent\n","*   a1.sources = r1\n","*   a1.sinks = k1\n","*   a1.channels = c1\n","\n","Describe and configure the source\n","*   a.sources.r1.type = spoolDir # list for file and pushes them to an HDFS Sink keeping tract of the time\n","*   a.sources.r1.spoolDir = /home/maria_dev/spool\n","*   a.sources.r1.fileHeader = true\n","*   a.sources.r1.interceptors = timestampInterceptor\n","*   a.sources.r1timestampIntercpetor.type = timestamp\n","\n","Describe the sink\n","*   a1.sinks.k1.type = hdfs # writes everything to hdfs from flume\n","*   a1.sink.k1.hdfs.path = /user/maria_dev/flume/%y-%m-%d/%H%M/%S\n","*   a1.sink.k1.hdfs.filePrefix = events-\n","*   a1.sink.k1.hdfs.round = true\n","*   a1.sink.k1.hdfs.roundValue=10\n","*   a1.sink.k1.hdfs.roundUnit=minute\n","\n","Use a channel which buffers events in memory\n","*   a1.channels.c1.type = memory\n","*   a1.channels.c1.capacity = 1000\n","*   a1.channels.c1.transactionCapacity = 100\n","\n","Bind the source and the sink to the channel\n","*   a1.sources.r1.channels = c1\n","*   a1.sink.k1.channel = c1\n"]},{"cell_type":"markdown","metadata":{"id":"4cmL8ftOFOJw"},"source":["### Since i have indicated using the spoolDir, I need to create it.\n","*   mkdir spool\n","*   then setup folder on HDFS through ambari\n","\n","Time to kick of flume\n","\n","*   cd /usr/hdp/current/flume-server/\n","*   bin/flume-ng agent --conf conf --conf-file ~/flumelogs.conf --name a1 -Dflume.root.logger=INFO, console #this means that every single logger and information that comes through is going to be print out."]},{"cell_type":"markdown","metadata":{"id":"RbJ51Yi_hfw6"},"source":["## Analuse web logs published with Flume using Spark Stream\n"," \n"," I need to do the config file:\n"," A single-node Flume configuration(setting up a flow)\n","Name the componenets on this agent\n","\n","*   a1.sources = r1\n","*   a1.sinks = k1\n","*   a1.channels = c1\n","\n","Describe and configure the source\n","*   a1.sources.r1.type = spoolDir # list for file and pushes them to an HDFS Sink keeping tract of the time\n","*   a1.sources.r1.spoolDir = /home/maria_dev/spool\n","*   a1.sources.r1.fileHeader = true\n","*   a1.sources.r1.interceptors = timestampInterceptor\n","*   a1.sources.r1timestampIntercpetor.type = timestamp\n","\n","Describe the sink\n","*   a1.sinks.k1.type = avro\n","*   a1.sink.k1.hostname = localhost\n","*   a1.sink.k1.port = 9091\n","\n","Use a channel which buffers events in memory\n","*   a1.channels.c1.type = memory\n","*   a1.channels.c1.capacity = 1000\n","*   a1.channels.c1.transactionCapacity = 100\n","\n","Bind the source and the sink to the channel\n","*   a1.sources.r1.channels = c1\n","*   a1.sink.k1.channel = c1\n"]},{"cell_type":"code","metadata":{"id":"itiJcwpBvIZ_"},"source":["# Spark Streaming script\n","\n","import re\n","\n","from pyspark import SparkContext\n","from pyspark.streaming import StreamingContext\n","from pyspark.streaming.flume import FlumeUtils\n","\n","parts = [\n","    r'(?P<host>\\S+)',                   # host %h\n","    r'\\S+',                             # indent %l (unused)\n","    r'(?P<user>\\S+)',                   # user %u\n","    r'\\[(?P<time>.+)\\]',                # time %t\n","    r'\"(?P<request>.+)\"',               # request \"%r\"\n","    r'(?P<status>[0-9]+)',              # status %>s\n","    r'(?P<size>\\S+)',                   # size %b (careful, can be '-')\n","    r'\"(?P<referer>.*)\"',               # referer \"%{Referer}i\"\n","    r'\"(?P<agent>.*)\"',                 # user agent \"%{User-agent}i\"\n","]\n","pattern = re.compile(r'\\s+'.join(parts)+r'\\s*\\Z')\n","\n","def extractURLRequest(line):\n","    exp = pattern.match(line)\n","    if exp:\n","        request = exp.groupdict()[\"request\"]\n","        if request:\n","           requestFields = request.split()\n","           if (len(requestFields) > 1):\n","                return requestFields[1]\n","\n","\n","if __name__ == \"__main__\":\n","\n","    sc = SparkContext(appName=\"StreamingFlumeLogAggregator\")\n","    sc.setLogLevel(\"ERROR\")\n","    ssc = StreamingContext(sc, 1)\n","\n","    flumeStream = FlumeUtils.createStream(ssc, \"localhost\", 9092)\n","\n","    lines = flumeStream.map(lambda x: x[1])\n","    urls = lines.map(extractURLRequest)\n","\n","    # Reduce by URL over a 5-minute window sliding every second\n","    urlCounts = urls.map(lambda x: (x, 1)).reduceByKeyAndWindow(lambda x, y: x + y, lambda x, y : x - y, 300, 1)\n","\n","    # Sort and print the results\n","    sortedResults = urlCounts.transform(lambda rdd: rdd.sortBy(lambda x: x[1], False))\n","    sortedResults.pprint()\n","\n","    ssc.checkpoint(\"/home/maria_dev/checkpoint\")\n","    ssc.start()\n","    ssc.awaitTermination()\n","pass"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4Zi-d3jlofxj"},"source":["mkdir checkpoint\n","spark-submit --packages org.apache.spark:spark-streaming-flume_2.11:2.0.0 sparkFlume.py #this would kick off the job\n","pass"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MuUDPBZL1rpO"},"source":["# kick of flume\n","*   cd /usr/hdp/current/flume-server/\n","*   bin/flume-ng agent --conf conf --conf-file ~/sparkstreamingflume.conf --name a1\n","pass"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"01SxghtF3Zx_"},"source":["#starting up another window to put something in there\n","wget http://media.sundog-soft.com/hadoop/access_log.txt\n","# by runningn this flume will pick it up and show it on the window\n","pass"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wHL1Kpm9jRGx"},"source":["### Aggregating http request with a five minute window and with a slide interval of 5 Seconds\n"]},{"cell_type":"code","metadata":{"id":"rzlhFX9C4pa1"},"source":["import re\n","\n","from pyspark import SparkContext\n","from pyspark.streaming import StreamingContext\n","from pyspark.streaming.flume import FlumeUtils\n","\n","parts = [\n","    r'(?P<host>\\S+)',                   # host %h\n","    r'\\S+',                             # indent %l (unused)\n","    r'(?P<user>\\S+)',                   # user %u\n","    r'\\[(?P<time>.+)\\]',                # time %t\n","    r'\"(?P<request>.+)\"',               # request \"%r\"\n","    r'(?P<status>[0-9]+)',              # status %>s\n","    r'(?P<size>\\S+)',                   # size %b (careful, can be '-')\n","    r'\"(?P<referer>.*)\"',               # referer \"%{Referer}i\"\n","    r'\"(?P<agent>.*)\"',                 # user agent \"%{User-agent}i\"\n","]\n","pattern = re.compile(r'\\s+'.join(parts)+r'\\s*\\Z')\n","\n","def extractURLRequest(line):\n","    exp = pattern.match(line)\n","    if exp:\n","        status = exp.groupdict()[\"status\"]\n","        if status:\n","          return status\n","\n","\n","if __name__ == \"__main__\":\n","\n","    sc = SparkContext(appName=\"StreamingFlumeLogAggregator\")\n","    sc.setLogLevel(\"ERROR\")\n","    ssc = StreamingContext(sc, 5)\n","\n","    flumeStream = FlumeUtils.createStream(ssc, \"localhost\", 9092)\n","\n","    lines = flumeStream.map(lambda x: x[1])\n","    urls = lines.map(extractURLRequest)\n","\n","    # Reduce by http over a 5-minute window sliding every second\n","    statusCounts = status.map(lambda x: (x, 1)).reduceByKeyAndWindow(lambda x, y: x + y, lambda x, y : x - y, 300, 1)\n","\n","    # Sort and print the results\n","    sortedResults = urlCounts.transform(lambda rdd: rdd.sortBy(lambda x: x[1], False))\n","    sortedResults.pprint()\n","\n","    ssc.checkpoint(\"/home/maria_dev/checkpoint\")\n","    ssc.start()\n","    ssc.awaitTermination()\n","pass"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IJcCx0YolpzZ"},"source":["## Apache Storm\n","*   Another framework for processing continuos streams of data on a cluster\n","  *   Can run on top of Yarn(like Spark)\n","*   Works on individual events, not micro-batch(like spark stream does)\n","  *   if you need sub-second latency Storm is appropriate.\n","### Storm terminology\n","*   A stream consists of tuples that flow through\n","*   Spouts that are sources of stream data(kafka, twitter, etc)\n","*   Bolts that process stream data as it is recieved\n","  *   Transform, aggregate, write to database/HDFS\n","* A topology is a graph of spouts and bolts that process your stream"]},{"cell_type":"markdown","metadata":{"id":"ETXgQ-HFuQty"},"source":["## Flink\n","*   German for quick and nimble\n","*   Another stream processing engine - most similar to storm\n","*   Can run on standalne cluster, or on top of Yarn or Mesos\n","*   Highly scalable(1000's of node)\n","*   Fault-Tolerant\n","  *   Can survive failures while guaranteeing exactly-once processing\n","  *   Uses \"state snapshots\" to achieve this.\n","* Up and coming quickly"]},{"cell_type":"markdown","metadata":{"id":"ayeGfp5I8W5M"},"source":["# Designing Real-World System\n","\n","## Working Backwards\n","*   Start from the end user's needs, not from where your data is coming from.\n","  *   Sometimes you may need to meet in the middle.\n","*   What sort of access pattern do you expect from the user?\n","  *   Analytical queries that span large data ranges?\n","  *   Huge amount of small transactions for very specific rows of data?\n","  *   Both?\n","*   What avalialability do these end users demand?\n","*   What consistency do these end users demand?  \n","\n","## Thinking about Requirements\n","*   Just  how Big is your Big Data?\n","  *   DO you really need a cluster?\n","*   How much internal infrastructure and experties is available?\n","  *   Should you use AWS or something similar?\n","  *   DO systems you already know fit the bill?\n","*   What about data retention?\n","  *   Do you need to keep data around forever, for auditing?\n","  *   Or do you need o purge it for privacy?\n","*   WHat about security?\n","  *   Check with legal\n","*   Latency\n","  *   How quickly do end users need to get a responce?\n","    *   Millisecond? Then something like Cassandra or HBase will be nedded.\n","*   Timelines\n","  *   Can quiries be based upon day-old-data, minute-old?\n","    *   Oozie schedule jobs in Hive, Pig, Spark, etc may cut it.\n","  *   Or it must be near-real-time\n","    *   Use Spark Streaming /Storm /Flink with kafka or flume\n","\n","## Judicious Future-Proofing\n","*   Once you have decided where to store your data, moving it later on will be difficult\n","  *   Think carefully before chooseing proprietry solutions or cloud-based solution.\n","*   WIll business analyst want you data in addition to end users)or vice versa)\n","\n","## Cheat to win\n","*   Does your organisation have existing components you can use?\n","  *   Don't build a datawarehouse if you already have one.\n","  *   Rebuild exiting technology always has negative business value.\n","*   What's the least amount of infrastructure you need to build?\n","  *   Import existing data with scoop, etc if you can\n","  *   If relaxing a requirement saves time and money, atleast ask."]},{"cell_type":"code","metadata":{"id":"HCWK8YFflmEJ"},"source":[""],"execution_count":null,"outputs":[]}]}